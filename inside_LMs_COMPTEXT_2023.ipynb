{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8olZ5vLT58Ca+AAS+vbDI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoritzLaurer/transformers-workshop-comptext-2023/blob/master/inside_LMs_COMPTEXT_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYLjU9bVQiI9"
      },
      "source": [
        "# 🤖 A pratical look inside of Transformers\n",
        "\n",
        "📅 _COMPTEXT 2023 tutorial, 11.05.2023_\n",
        "\n",
        "👨‍🏫 By [Moritz Laurer](https://twitter.com/MoritzLaurer). \n",
        "For questions, reach out to: m.laurer@vu.nl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer's main components"
      ],
      "metadata": {
        "id": "0-JnK7Zkhmj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hugging Face Transformers has two main components:**\n"
      ],
      "metadata": {
        "id": "u2v9wkhQhuf4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHkp89twf4Zq"
      },
      "source": [
        "\n",
        "1. The **tokenizer** prepares the text in a clean format, which the model understands. \n",
        "    - A token is a word or a sub-word unit. In BERT's vocabulary, the word \"good\" is one token and the word \"darwinism\" is two tokens  (\"darwin\" and \"ism\")\n",
        "    - The tokenizer transforms words into token-ids. With these token-ids, BERT can link words to any token it has already learned during pre-training. \n",
        "\n",
        "2. The **model** processes the tokenizer's ouput and returns a prediction, e.g. which class an input text belongs to.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independently of the type of model (classification, summarisation, translation, etc.), these two components are almost the same."
      ],
      "metadata": {
        "id": "A4tmhG0piJ0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and load "
      ],
      "metadata": {
        "id": "Gir0-XiAhhfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers~=4.26.1  # The Transformers library from Hugging Face"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNjndqEeVP5h",
        "outputId": "e096e38a-8f1b-431d-c044-01858f6307b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers~=4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers~=4.26.1) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.26.1) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.26.1) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers~=4.26.1) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.26.1) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.26.1) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.26.1) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers~=4.26.1) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers~=4.26.1) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.26.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.26.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.26.1) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.26.1) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hykl2GkBhq5F"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HneEleBYh_tY"
      },
      "source": [
        "# load any classification model from the HuggingFace model hub\n",
        "# See here: https://huggingface.co/models?pipeline_tag=text-classification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# instantiate the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# instantiate the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zluWT2EpHd64"
      },
      "source": [
        "### Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLKD6HULf4hX",
        "outputId": "e674039b-7dbe-4002-ee26-92fbd11bb591"
      },
      "source": [
        "### 1. Tokenization\n",
        "# Tokenizer documentation: https://huggingface.co/transformers/main_classes/tokenizer.html\n",
        "\n",
        "text = 'I believe that the EU is trustworthy.'\n",
        "print(f\"Input text: '{text}'\\n\")\n",
        "\n",
        "input_ids = tokenizer(text, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "print(f\"\"\"The tokenizer splits the text string into separate tokens. A token is either an entire word,\n",
        "or a 'sub-word unit' in case of rare words (or punctuation).\n",
        "The word 'trustworthy', for example is split into two tokens: {tokenizer.tokenize(\"Trustworthy\")}.\n",
        "The main advantage of these sub-word units is that rare words cannot be out-of-vocabulary (an issue of other text-as-data approaches).\n",
        "Transformer models typically have a vocabulary of around 30.000 - 250.000 tokens, learned from the training data. \n",
        "Here is e.g. the vocabulary of DistilBERT: https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt\\n\"\"\")\n",
        "\n",
        "print(f\"The input text is split into the following tokens:\\n{tokenizer.tokenize(text)}.\")\n",
        "print(\"The tokenizer then maps each token to the corresponding token ID in the model's vocabulary:\")\n",
        "print(input_ids[0].tolist()[1:-1])\n",
        "print(\"Transformer models only understand these token IDs.\\n\")\n",
        "\n",
        "print(\"\"\"In addition, the tokenizer adds two special tokens:\n",
        " First, the [CLS] (classification) token is always added at the beginning. \n",
        "        While individual tokens represent individual (sub)words, the [CLS] token represents the entire text. \n",
        "        The [CLS] token \"is  used  as  the  aggregate sequence representation for classification tasks\" (Devlin et al. 2019: 4). Details: https://arxiv.org/pdf/1810.04805.pdf\n",
        " Second, the [SEP] token separates two texts. It is useful for tasks which require two text inputs, for example Questions & Answer tasks.\n",
        "        (It is not relevant in our case)\n",
        "\\n\"\"\")\n",
        "\n",
        "print(\"\"\"The final input for a BERT transformer model therefore looks like this:\"\"\")\n",
        "token_strings = tokenizer.convert_ids_to_tokens(ids=input_ids[0])\n",
        "#token_strings = tokenizer.tokenize(text)\n",
        "for token_id, token_string in zip(input_ids[0].tolist(), token_strings):\n",
        "  print(token_id, \" == \", token_string)\n",
        "\n",
        "\n",
        "# entire vocabulary: tokenizer.pretrained_vocab_files_map[\"vocab_file\"][\"distilbert-base-uncased\"]\n",
        "# https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: 'I believe that the EU is trustworthy.'\n",
            "\n",
            "The tokenizer splits the text string into separate tokens. A token is either an entire word,\n",
            "or a 'sub-word unit' in case of rare words (or punctuation).\n",
            "The word 'trustworthy', for example is split into two tokens: ['trust', '##worthy'].\n",
            "The main advantage of these sub-word units is that rare words cannot be out-of-vocabulary (an issue of other text-as-data approaches).\n",
            "Transformer models typically have a vocabulary of around 30.000 - 250.000 tokens, learned from the training data. \n",
            "Here is e.g. the vocabulary of DistilBERT: https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt\n",
            "\n",
            "The input text is split into the following tokens:\n",
            "['i', 'believe', 'that', 'the', 'eu', 'is', 'trust', '##worthy', '.'].\n",
            "The tokenizer then maps each token to the corresponding token ID in the model's vocabulary:\n",
            "[1045, 2903, 2008, 1996, 7327, 2003, 3404, 13966, 1012]\n",
            "Transformer models only understand these token IDs.\n",
            "\n",
            "In addition, the tokenizer adds two special tokens:\n",
            " First, the [CLS] (classification) token is always added at the beginning. \n",
            "        While individual tokens represent individual (sub)words, the [CLS] token represents the entire text. \n",
            "        The [CLS] token \"is  used  as  the  aggregate sequence representation for classification tasks\" (Devlin et al. 2019: 4). Details: https://arxiv.org/pdf/1810.04805.pdf\n",
            " Second, the [SEP] token separates two texts. It is useful for tasks which require two text inputs, for example Questions & Answer tasks.\n",
            "        (It is not relevant in our case)\n",
            "\n",
            "\n",
            "The final input for a BERT transformer model therefore looks like this:\n",
            "101  ==  [CLS]\n",
            "1045  ==  i\n",
            "2903  ==  believe\n",
            "2008  ==  that\n",
            "1996  ==  the\n",
            "7327  ==  eu\n",
            "2003  ==  is\n",
            "3404  ==  trust\n",
            "13966  ==  ##worthy\n",
            "1012  ==  .\n",
            "102  ==  [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokens (words) following through the neural network"
      ],
      "metadata": {
        "id": "rw7qQhjGcZ8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions: \n",
        "- Can someone explain what a word-embedding is?"
      ],
      "metadata": {
        "id": "JbkkFf7qYHYR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z8vNYtliIaJ",
        "outputId": "64404cd8-c2c0-4a0d-d1b9-ddb7b1c72aa0"
      },
      "source": [
        "### Processing the input with the model\n",
        "# Model class documentation: https://huggingface.co/transformers/main_classes/model.html\n",
        "# Documentation for DistilBERT specifically: https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "print(f\"\"\"\\nAfter the preprocessing by the tokenizer, the model then feeds the sequence of tokens through the neural network.\n",
        "Each token is represented by a vector of 768 numbers (a 768 dimensional tensor).\n",
        "The tensor for the token \"trust\" looks for example like this before being fed into the first neural network layer \n",
        "(only 100 numbers are displayed):\\n\"\"\")\n",
        "print(model.distilbert.embeddings.word_embeddings(input_ids[0][7])[:100], \"\\n\")\n",
        "\n",
        "print(f\"\"\"The tensors for each token are then fed through and transformed by between 6-24~ neural network layers.\\n\"\"\")\n",
        "\n",
        "output = model(input_ids, output_hidden_states=True, output_attentions=False, return_dict=True)\n",
        "print(\"Same word after the first layer:\\n\\n\", output.hidden_states[1][0][7][:100], \"\\n\")  # same word embedding after the first attention layer\n",
        "print(\"Same word after the second layer:\\n\\n\", output.hidden_states[2][0][7][:100], \"\\n\")  # same word embedding after the second attention layer\n",
        "#print(\"Same word after the third layer:\\n\", output.hidden_states[3][0][7][:100], \"\\n\")  # same word embedding after the third attention layer\n",
        "print(\"\\n ... etc ...\\n\")\n",
        "\n",
        "print(f'The final output is a a contextualised representation of the sequence: \"{text}\"')\n",
        "#output.hidden_states[6][0][0][:100]  # final CLS token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After the preprocessing by the tokenizer, the model then feeds the sequence of tokens through the neural network.\n",
            "Each token is represented by a vector of 768 numbers (a 768 dimensional tensor).\n",
            "The tensor for the token \"trust\" looks for example like this before being fed into the first neural network layer \n",
            "(only 100 numbers are displayed):\n",
            "\n",
            "tensor([-0.0263, -0.0292, -0.0186,  0.0289,  0.0225,  0.0005, -0.0649,  0.0440,\n",
            "         0.0201,  0.0052, -0.0857, -0.0903, -0.0182, -0.0214, -0.0514, -0.0074,\n",
            "        -0.0361, -0.0715,  0.0125, -0.0320, -0.0118, -0.0252, -0.0431, -0.0383,\n",
            "         0.0073,  0.0188,  0.0049, -0.0829, -0.0150, -0.0313, -0.0517,  0.0518,\n",
            "         0.0099,  0.0418, -0.0135, -0.0256, -0.0432, -0.0029, -0.0191,  0.0006,\n",
            "         0.0023,  0.0052, -0.0705, -0.0053, -0.0237, -0.0131,  0.0082, -0.0160,\n",
            "        -0.0512,  0.0171,  0.0104, -0.0164, -0.0536, -0.0759, -0.0407, -0.0006,\n",
            "        -0.0331, -0.0792,  0.0354, -0.0010, -0.0222, -0.0015, -0.0628, -0.0206,\n",
            "        -0.1149, -0.0215, -0.0275, -0.0074, -0.0037,  0.0314, -0.0694, -0.0056,\n",
            "        -0.0402, -0.0959, -0.1495, -0.0860, -0.0756, -0.0939, -0.0162,  0.0127,\n",
            "        -0.0333, -0.0174, -0.0606, -0.0488,  0.0328, -0.0653, -0.0148,  0.0429,\n",
            "        -0.0382,  0.0021,  0.0154, -0.0832, -0.0312,  0.0751, -0.0100, -0.0326,\n",
            "         0.0089, -0.0080, -0.0745, -0.0516], grad_fn=<SliceBackward0>) \n",
            "\n",
            "The tensors for each token are then fed through and transformed by between 6-24~ neural network layers.\n",
            "\n",
            "Same word after the first layer:\n",
            "\n",
            " tensor([ 1.0197, -0.5487,  0.3499,  0.4607,  0.8391, -0.1893, -1.2744,  0.4050,\n",
            "         0.5758,  0.5190, -1.3808, -0.4710,  0.1655, -0.9737, -0.9132,  0.0945,\n",
            "         0.1522, -0.7325,  0.4531, -0.1877, -0.0658,  0.5347, -0.2890,  0.2111,\n",
            "         0.0095,  1.8999,  0.3661, -0.1923, -0.1967,  0.0840, -0.9437,  0.6535,\n",
            "         0.8659,  1.1503,  0.5523, -0.0571, -0.5224,  0.5320,  0.4575,  0.0705,\n",
            "         0.5444,  0.6075, -0.5054,  0.6493, -0.7816, -0.1386,  0.9620,  0.1940,\n",
            "         0.2520,  0.2409,  0.3830, -0.0106, -0.9761, -0.4375, -0.1450,  0.3104,\n",
            "        -0.4289, -0.2048,  0.7036,  0.0452,  0.3296,  0.4499,  0.4429, -0.7283,\n",
            "        -2.3567,  0.3876, -0.9062,  0.0851, -0.0852,  0.0829, -0.8152, -0.3009,\n",
            "        -0.6328, -1.0584, -2.2323, -0.9247, -1.3073, -0.2668,  0.3371,  0.0062,\n",
            "         0.2274, -0.0837, -1.3364, -0.1008,  0.2650, -1.4553,  0.6272,  0.5761,\n",
            "        -0.1431, -0.0612, -0.0905, -0.8610, -0.4050,  1.5188,  0.6204, -0.2890,\n",
            "         0.3655,  0.1397, -1.0399, -0.5957], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Same word after the second layer:\n",
            "\n",
            " tensor([ 8.6304e-01, -8.6820e-01,  7.8730e-01, -2.2138e-01,  1.2101e+00,\n",
            "         3.8670e-01, -1.0896e+00,  8.7422e-02,  4.7191e-01,  3.4434e-02,\n",
            "        -1.0020e+00,  6.2982e-01, -1.2089e-01, -5.2946e-01, -1.1913e+00,\n",
            "        -4.6408e-01,  1.9621e-02, -7.6431e-01, -4.3572e-01, -2.6467e-01,\n",
            "         1.8397e-01,  1.6879e-01,  3.4552e-02,  3.8005e-01, -1.0561e+00,\n",
            "         1.5122e+00,  8.2248e-01,  1.1769e-02,  6.4608e-02,  7.2765e-01,\n",
            "        -6.4231e-01, -1.5865e-02,  3.4215e-01,  1.7413e+00,  4.9454e-01,\n",
            "        -5.1311e-01, -3.9811e-01,  3.1635e-01,  9.2214e-01, -5.0008e-02,\n",
            "        -1.1514e-01,  6.3453e-01, -2.1228e-02,  5.0645e-01, -3.6516e-01,\n",
            "         9.9460e-02,  5.1015e-01, -2.4458e-01, -7.1837e-02, -1.2050e-01,\n",
            "        -1.2111e-01, -5.2317e-01, -9.3992e-01, -3.8500e-01,  1.9844e-01,\n",
            "        -7.1985e-01, -1.7236e+00, -2.9698e-01,  5.4476e-01,  6.2087e-01,\n",
            "         1.1353e+00,  4.7976e-02,  7.7644e-02, -1.1994e+00, -1.6474e+00,\n",
            "        -8.3197e-02, -7.1110e-01,  4.4613e-01, -1.4741e-01, -1.5836e-02,\n",
            "         9.5763e-04, -1.7963e-01, -2.0893e-01, -8.2586e-01, -1.6915e+00,\n",
            "        -3.0478e-01, -7.3935e-01, -3.4275e-01,  7.6856e-01, -1.3046e-01,\n",
            "         6.7788e-01, -1.4471e-01, -1.2505e+00, -2.1345e-01, -2.3939e-01,\n",
            "        -1.3216e+00,  1.0545e+00, -1.3779e-01, -2.7386e-01,  3.0029e-01,\n",
            "        -6.4045e-01, -9.0891e-01,  1.5595e-01,  1.4556e+00,  4.6438e-01,\n",
            "        -8.5181e-02,  5.4725e-01,  3.0987e-01, -6.2029e-01, -6.2813e-01],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "\n",
            " ... etc ...\n",
            "\n",
            "The final output is a a contextualised representation of the sequence: \"I believe that the EU is trustworthy.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualisation of this process:\n",
        "# TODO: add image from .ppt"
      ],
      "metadata": {
        "id": "-BLywJBioBRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey7iZ13DsoKa",
        "outputId": "f3212f58-f6c4-4a51-d3f3-077de2cbe3bb"
      },
      "source": [
        "print(\"This is what the different model layers ('the architecture') look like:\\n\")\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is what the different model layers look like:\n",
            "\n",
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (1): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (2): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (3): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (4): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (5): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The final output"
      ],
      "metadata": {
        "id": "SNThz6Mdc6Ei"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1ecRDjcsh-Z",
        "outputId": "a2a25d99-6ede-447c-f57d-5cf8c0ffec17"
      },
      "source": [
        "print(f\"\"\"At the end, Transformer models always output so called 'logits', one number for each class the model was trained to classify.\\n\n",
        "These logis represent the predicted probability for our binary classification task:\\n\\n{output[\"logits\"][0].tolist()}\\n\"\"\")\n",
        "\n",
        "print(\"Logits are not very interpretable, so they are then converted to percentages.\\nEach percentages represents the model's prediction, which class the input text belongs to.\\n\")\n",
        "probabilities = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
        "label_names = model.config.id2label.values()\n",
        "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(probabilities, label_names)}\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At the end, Transformer models always output so called 'logits', one number for each class the model has learned to classify.\n",
            "\n",
            "These logis represent the predicted probability for our binary classification task:\n",
            "\n",
            "[-3.50547456741333, 3.680955171585083]\n",
            "\n",
            "Logits are not very interpretable, so they are then converted to percentages.\n",
            "Each percentages represents the model's prediction, which class the input text belongs to.\n",
            "\n",
            "{'NEGATIVE': 0.1, 'POSITIVE': 99.9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_AJJsLQHq3l"
      },
      "source": [
        "### Everything put together\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZK019KRAxHm",
        "outputId": "f6d37c5a-726e-487a-de1b-dc23b1ed48c4"
      },
      "source": [
        "## In short, the code looks like this:\n",
        "\n",
        "# load the relevant functions from HuggingFace and PyTorch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Choose any classification model from the model hub\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# instantiate the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# tokenization\n",
        "text = 'I believe that the EU is trustworthy.'\n",
        "input = tokenizer(text, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# model prediction\n",
        "output = model(input, output_hidden_states=False, output_attentions=False, return_dict=True)\n",
        "probabilities = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
        "label_names = model.config.id2label.values()\n",
        "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(probabilities, label_names)}\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NEGATIVE': 0.1, 'POSITIVE': 99.9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrJNiO8HBX8v",
        "outputId": "bd92846d-3b7d-468f-91e2-52af9ad7d862"
      },
      "source": [
        "## Or via the simplified pipeline: \n",
        "from transformers import pipeline\n",
        "pipe_classification = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", top_k=2)\n",
        "text = 'I believe that the EU is trustworthy.'\n",
        "pipe_classification(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'POSITIVE', 'score': 0.9992438554763794},\n",
              "  {'label': 'NEGATIVE', 'score': 0.0007562137907370925}]]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}